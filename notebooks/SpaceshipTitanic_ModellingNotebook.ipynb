{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae97ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78af75a9",
   "metadata": {},
   "source": [
    "1. Below is a code cell, purpose of which is confirming basic stats like Dataframe shape and the proportion of Missing values in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2174a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = pd.read_csv('../data/raw/train.csv')\n",
    "test = pd.read_csv('../data/raw/test.csv')\n",
    "\n",
    "train_raw = train.copy()\n",
    "test_raw = test.copy()\n",
    "\n",
    "train_pipeline = train_raw.copy()\n",
    "test_pipeline = test_raw.copy()\n",
    "\n",
    "print(f'Train Shape: {train.shape}')\n",
    "print(f'Test Shape: {test.shape}')\n",
    "\n",
    "display(train.head())\n",
    "display(train.isna().mean().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae948a8",
   "metadata": {},
   "source": [
    "Step 2 Data Analysis and Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5e7d00",
   "metadata": {},
   "source": [
    "2.1 Missing Value Overview\n",
    "To accurately clean data, it's needed to know proportion of missing data of each column. From running the only full cols are Passenger ID and whether they made it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecabe329",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_frac = train.isna().mean()\n",
    "missing_frac = missing_frac.sort_values(ascending= False)\n",
    "print(missing_frac)\n",
    "full_cols = missing_frac[missing_frac == 0].index.to_list()\n",
    "#print(full_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005e1a6a",
   "metadata": {},
   "source": [
    "2.2 Inspection of indivual columns and their distributions: range and obvious outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856d209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['Age', 'Spa', 'FoodCourt','RoomService','VRDeck','ShoppingMall']\n",
    "print('Numeric Column summaries:')\n",
    "display(train[numeric_cols].describe())\n",
    "\n",
    "cat_cols = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP']\n",
    "for col in cat_cols:\n",
    "    print(f'{col}')\n",
    "    print(train[col].value_counts(dropna= False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e283ee18",
   "metadata": {},
   "source": [
    "2.3 Univariate Analysis: Numerical Features and Categorical Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3ad6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for col in numeric_cols:\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2,figsize=(12,4))\n",
    "    \n",
    "    #Histogram\n",
    "    ax1.hist(train[col].dropna(), bins = 30)\n",
    "    ax1.set_title(f'{col} distribution')\n",
    "    ax1.set_label(col)\n",
    "    \n",
    "    #Boxplot \n",
    "    ax2.boxplot(train[col].dropna(), vert=False)\n",
    "    ax2.set_title(f'{col} â€” boxplot')\n",
    "    ax2.set_xlabel(col)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    missing_pct = missing_frac[col] * 100\n",
    "    print(f'{col}: {missing_pct:.1f}% missing\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab0d510",
   "metadata": {},
   "source": [
    "2.4 Bivariate Analysis for both categorical and numerical features\n",
    "First Code box shows the count plot for the categorical data and then splits the count into Transported vs Not based on the categorical column its in.\n",
    "Second box is for numerical data shows the distribution of transported vs Not of the specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd46c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2,figsize = (12,5))\n",
    "    sns.countplot(data = train, x = col, ax = ax1)\n",
    "    ax1.set_title(f'Passenger Count by {col}')\n",
    "    \n",
    "    \n",
    "    sns.countplot(data=train, x= col, hue='Transported', ax = ax2)\n",
    "    ax1.set_title(f'Transported Status by {col}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c7a8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numeric_cols:\n",
    "    sns.boxplot(data = train, x = 'Transported', y = col)\n",
    "    plt.title(f'{col} Distribution by Transported Status')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c2a003",
   "metadata": {},
   "source": [
    "3. Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e580a8f8",
   "metadata": {},
   "source": [
    "3.1 Missing Data Handling and Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e091f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_medians = {col: train_raw[col].median() for col in numeric_cols}\n",
    "cat_modes = {col: train_raw[col].mode().iloc[0] for col in cat_cols}\n",
    "\n",
    "for df in [train,test]:\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(value= numeric_medians)\n",
    "    df[cat_cols] = df[cat_cols].fillna(value = cat_modes)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868e6711",
   "metadata": {},
   "source": [
    "3.2 Feature Engineering and imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ca3d8a",
   "metadata": {},
   "source": [
    "Splits Cabin into Deck, Room Number and side of ship. Also makes Total Spend column and update the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f168fb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = ['Deck','Num','Side']\n",
    "spending_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "\n",
    "for df in [train,test]:\n",
    "    #Creation of group category which can be used to calculate group size outside of the for loop \n",
    "    df['TotalSpend'] = df[spending_cols].sum(axis = 1)\n",
    "    df['Group'] = df['PassengerId'].str.split('_').str[0]\n",
    "    \n",
    "    #Splitting of Cabin into Deck and Side \n",
    "    df[new_cols] = df['Cabin'].str.split('/', expand= True)\n",
    "\n",
    "\n",
    "cat_cols_extended = cat_cols + new_cols\n",
    "numeric_cols_extended = numeric_cols + ['TotalSpend']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7011f79e",
   "metadata": {},
   "source": [
    "Calculation of group size and addition to Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4e8d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_sizes = pd.concat([train['Group'], test['Group']]).value_counts().rename('GroupSize')\n",
    "\n",
    "train['GroupSize'] = train['Group'].map(group_sizes)\n",
    "test['GroupSize'] = test['Group'].map(group_sizes)\n",
    "\n",
    "numeric_cols_extended = numeric_cols_extended + ['GroupSize']\n",
    "cat_cols_extended.remove('Num')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b833a89",
   "metadata": {},
   "source": [
    "Drops Non-Needed columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72cdeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['Num','Group','PassengerId','Cabin']\n",
    "test_pass_ids = test['PassengerId']\n",
    "train_model = train.drop(columns= cols_to_drop)\n",
    "test_model = test.drop(columns= cols_to_drop)\n",
    "\n",
    "y = train_model['Transported'].astype(int)\n",
    "x = train_model.drop(columns=['Transported'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d951c117",
   "metadata": {},
   "source": [
    "4. Building and Evaluating a ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689303cc",
   "metadata": {},
   "source": [
    "4.1 Splitting train data into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b885388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_val,y_train,y_val = train_test_split(\n",
    "    x,y,\n",
    "    test_size= 0.2,\n",
    "    stratify = y,\n",
    "    random_state= 42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7414559d",
   "metadata": {},
   "source": [
    "4.2 Setting up a preprocessor for Categorical and Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c758206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "\n",
    "cat_feats = cat_cols_extended\n",
    "num_feats = numeric_cols_extended\n",
    "\n",
    "cat_transformer = OneHotEncoder(handle_unknown= 'ignore',sparse_output= False)\n",
    "num_transformer = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat',cat_transformer,cat_feats),\n",
    "    ('num',num_transformer,num_feats)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa6c197",
   "metadata": {},
   "source": [
    "4.3 Testing different Models for the pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a10fca",
   "metadata": {},
   "source": [
    "Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef9d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score,roc_auc_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "def eval_model(model,x,y):\n",
    "    y_pred = cross_val_predict(model,x,y, cv =5)\n",
    "    y_proba = cross_val_predict(model,x,y, cv=5, method = 'predict_proba')[:,1] #Probability of y being True (1)\n",
    "    \n",
    "    accuracy = accuracy_score(y,y_pred)\n",
    "    F1 = f1_score(y,y_pred)\n",
    "    ROC = roc_auc_score(y,y_proba)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f} | F1: {F1:.4f} | ROC AUC: {ROC:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7079dd3",
   "metadata": {},
   "source": [
    "\n",
    "Models and Comparison of each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e238592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor',preprocessor),\n",
    "    ('classifier',None)\n",
    "])\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=200),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'XGBoost': XGBClassifier( eval_metric='logloss'),\n",
    "    'LightGBM': LGBMClassifier(verbose = -1),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Dummy Classifier' : DummyClassifier(strategy='most_frequent')\n",
    "}\n",
    "\n",
    "for model, clf in models.items():\n",
    "    pipeline.set_params(classifier = clf)\n",
    "    print(model)\n",
    "    eval_model(pipeline,x,y)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810058d9",
   "metadata": {},
   "source": [
    "4.4 Tuning of Parameters for the Gradient Boosting Model (best performance)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fdc416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators':[], #Number of Trees (boosting rounds) in prediction model \n",
    "    'learning_rate':[], #Changes weight of contribution from each tree \n",
    "    'max_depth':[], #Controls depth of individual trees \n",
    "    'subsample':[], #Fraction of samples used per boosting round (lowers overfitting)\n",
    "    'min_sample_split':[] #Min samples to split a node\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
